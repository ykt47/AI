{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb04b82e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Food Review Classification\n",
    "# Group Members:Teh Wei Zhang,Chai Ee Yuan,Tang Yik Hong\n",
    "\n",
    "Sentiment analysis is a natural language processing (NLP) method used to detect the emotional tone of written text. In food reviews, it helps identify whether customer feedback is positive, negative, or neutral. For instance, praise for taste or service reflects positive sentiment, while complaints about price or delays indicate negative sentiment. Neutral reviews may simply describe portion size or menu variety without strong emotion.\n",
    "\n",
    "By turning qualitative opinions into structured data, sentiment analysis reveals broader patterns in customer satisfaction. These insights allow restaurants and food platforms to track strengths, address weaknesses, and improve overall dining experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177e6f7",
   "metadata": {},
   "source": [
    "# Benefits of sentiment analysis\n",
    "\n",
    "1. **Understanding Customer Opinions** It helps businesses quickly identify whether customer feedback is positive, negative, or neutral, giving a clear picture of satisfaction levels.\n",
    "\n",
    "2. **Improving Decision-Making**: By converting subjective comments into measurable data, organizations can make data-driven choices about product quality, pricing, or service improvements.\n",
    "\n",
    "3. **Real-Time Analysis**: By identifying critical issues in real-time, sentiment analysis allows businesses to detect emerging patterns in audience reception immediately after release. This enables production companies to quickly identify pain points and implement strategic responses to audience feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2cae68f",
   "metadata": {
    "id": "A7skSh1jcGtV",
    "outputId": "9c4d7069-caf7-4ed4-e04e-482706d838e0",
    "papermill": {
     "duration": 0.019755,
     "end_time": "2023-11-25T15:22:08.350907",
     "exception": false,
     "start_time": "2023-11-25T15:22:08.331152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec05bd",
   "metadata": {
    "id": "cgslNYzaru6y",
    "papermill": {
     "duration": 0.005186,
     "end_time": "2023-11-25T15:22:08.361950",
     "exception": false,
     "start_time": "2023-11-25T15:22:08.356764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Description\n",
    "\n",
    "This project utilizes the Amazon Fine Food Reviews Dataset available on Kaggle, a large-scale collection widely used in sentiment analysis research. The dataset contains over 500,000 food-related reviews from Amazon, including detailed ratings, review text, and metadata.\n",
    "\n",
    "Dataset structure:\n",
    "- Reviews: More than 500,000 textual evaluations of food products on Amazon\n",
    "- Ratings: 1–5 star scale, which can be mapped into negative, neutral, and positive sentiment classes\n",
    "- Balance: For experimental purposes, subsets can be sampled to create equal distributions across sentiment categories\n",
    "- Source: https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
    "\n",
    "Our research objective is to systematically evaluate multiple machine learning approaches for sentiment classification, determining which methods most effectively predict customer sentiment from review text. This analysis will highlight the relative performance of classification strategies when applied to the domain-specific language patterns found in food product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b877068",
   "metadata": {
    "id": "QTcVNRdcfrAb",
    "outputId": "6fd5746d-85ae-433c-8da7-60bfdb738209",
    "papermill": {
     "duration": 2.14645,
     "end_time": "2023-11-25T15:22:10.514614",
     "exception": false,
     "start_time": "2023-11-25T15:22:08.368164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId  Rating                                               Text\n",
      "0   1  B001E4KFG0       5  I have bought several of the Vitality canned d...\n",
      "1   2  B00813GRG4       1  Product arrived labeled as Jumbo Salted Peanut...\n",
      "2   3  B000LQOCH0       4  This is a confection that has been around a fe...\n",
      "3   4  B000UA0QIQ       2  If you are looking for the secret ingredient i...\n",
      "4   5  B006K2ZZ7K       5  Great taffy at a great price.  There was a wid...\n",
      "Rating\n",
      "5    363103\n",
      "4     80650\n",
      "1     52265\n",
      "3     42605\n",
      "2     29768\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file = 'Reviews.csv'\n",
    "df_review = pd.read_csv(file, encoding=\"latin1\",low_memory=False) #latin1 = ISO-8859-1 , 1 type of word, Pandas reads the file in chunks to conserve memory.\n",
    "df_review = df_review.loc[:, ~df_review.columns.str.contains(r'^Unnamed')] #the unamed \n",
    "df_review = df_review[df_review['Rating'].between(1, 5)] #only display Rating that are 1-5 \n",
    "print(df_review.head())\n",
    "print(df_review['Rating'].value_counts())  # original column name in this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ed029",
   "metadata": {
    "id": "iTN-aaCNwo1H",
    "papermill": {
     "duration": 0.005998,
     "end_time": "2023-11-25T15:22:10.526393",
     "exception": false,
     "start_time": "2023-11-25T15:22:10.520395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating three separate datasets for positive,negative reviews\n",
    "\n",
    "We use the Amazon Fine Food Reviews dataset (500k+ rows). Ratings are mapped to sentiment as:\n",
    "- Negative = 1–2 stars\n",
    "- Neutral = 3 stars\n",
    "- Positive = 4–5 stars\n",
    "\n",
    "Using simple filtering in Pandas, we created three DataFrames: df_positive, df_negative, and df_neutral. Each contains only the reviews belonging to its sentiment class. Printing the head of each subset confirms that the split worked correctly and shows sample reviews from every category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c18429ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "positive    443753\n",
      "negative     82033\n",
      "neutral      42605\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def map_sentiment(rating):\n",
    "    if rating < 3:\n",
    "        return \"negative\"\n",
    "    elif rating == 3:\n",
    "        return \"neutral\"\n",
    "    else: \n",
    "        return \"positive\"\n",
    "\n",
    "df_review['sentiment'] = df_review['Rating'].apply(map_sentiment)\n",
    "\n",
    "print(df_review['sentiment'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2eaa3f",
   "metadata": {
    "id": "Lbk3KWq4jPwq",
    "outputId": "8b448090-860d-4fca-a880-09f65f2d9fc2",
    "papermill": {
     "duration": 0.033343,
     "end_time": "2023-11-25T15:22:10.565941",
     "exception": false,
     "start_time": "2023-11-25T15:22:10.532598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive examples:\n",
      "    Id   ProductId  Rating                                               Text  \\\n",
      "0   1  B001E4KFG0       5  I have bought several of the Vitality canned d...   \n",
      "2   3  B000LQOCH0       4  This is a confection that has been around a fe...   \n",
      "4   5  B006K2ZZ7K       5  Great taffy at a great price.  There was a wid...   \n",
      "5   6  B006K2ZZ7K       4  I got a wild hair for taffy and ordered this f...   \n",
      "6   7  B006K2ZZ7K       5  This saltwater taffy had great flavors and was...   \n",
      "\n",
      "  sentiment  \n",
      "0  positive  \n",
      "2  positive  \n",
      "4  positive  \n",
      "5  positive  \n",
      "6  positive  \n",
      "Negative examples:\n",
      "     Id   ProductId  Rating                                               Text  \\\n",
      "1    2  B00813GRG4       1  Product arrived labeled as Jumbo Salted Peanut...   \n",
      "3    4  B000UA0QIQ       2  If you are looking for the secret ingredient i...   \n",
      "12  13  B0009XLVG0       1  My cats have been happily eating Felidae Plati...   \n",
      "16  17  B001GVISJM       2  I love eating them and they are good for watch...   \n",
      "26  27  B001GVISJM       1  The candy is just red , No flavor . Just  plan...   \n",
      "\n",
      "   sentiment  \n",
      "1   negative  \n",
      "3   negative  \n",
      "12  negative  \n",
      "16  negative  \n",
      "26  negative  \n",
      "Neutral examples:\n",
      "     Id   ProductId  Rating                                               Text  \\\n",
      "45  46  B001EO5QW8       3  This seems a little more wholesome than some o...   \n",
      "47  48  B001EO5QW8       3  The flavors are good.  However, I do not see a...   \n",
      "49  50  B001EO5QW8       3  This is the same stuff you can buy at the big ...   \n",
      "53  54  B000G6RPMY       3  we're used to spicy foods down here in south t...   \n",
      "60  61  B004N5KULM       3  Watch your prices with this.  While the assort...   \n",
      "\n",
      "   sentiment  \n",
      "45   neutral  \n",
      "47   neutral  \n",
      "49   neutral  \n",
      "53   neutral  \n",
      "60   neutral  \n"
     ]
    }
   ],
   "source": [
    "df_positive = df_review[df_review['sentiment']=='positive']\n",
    "df_negative = df_review[df_review['sentiment']=='negative']\n",
    "df_neutral  = df_review[df_review['sentiment']=='neutral']\n",
    "\n",
    "print(\"Positive examples:\\n\", df_positive.head())\n",
    "print(\"Negative examples:\\n\", df_negative.head())\n",
    "print(\"Neutral examples:\\n\", df_neutral.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d8d81",
   "metadata": {
    "id": "zb-JQhibwwn7",
    "papermill": {
     "duration": 0.005438,
     "end_time": "2023-11-25T15:22:10.577303",
     "exception": false,
     "start_time": "2023-11-25T15:22:10.571865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Random sampling to generate balanced dataset\n",
    "\n",
    "The Amazon Fine Food Reviews dataset is highly imbalanced, with many more positive (5-star) reviews compared to neutral and negative ones. To address this, we used random sampling to create a dataset with a controlled number of reviews from each sentiment category.\n",
    "\n",
    "- 100,000 positive reviews\n",
    "\n",
    "- 80,000 negative reviews\n",
    "\n",
    "- 40,000 neutral reviews\n",
    "\n",
    "We then combined these subsets into a single DataFrame and shuffled the rows to mix the classes evenly.\n",
    "\n",
    "***dataset_name.sample(n = no_of_rows)***\n",
    "\n",
    "We can randomly select a specified number of rows from each sentiment category. This ensures that the final dataset maintains a distribution closer to balance across all three classes, which is important for fair training and evaluation of sentiment analysis models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea46d31",
   "metadata": {
    "id": "SWCJTyuFjezd",
    "outputId": "c66d8acc-af79-44b5-eacf-d1e7d3f920d3",
    "papermill": {
     "duration": 0.02263,
     "end_time": "2023-11-25T15:22:10.605789",
     "exception": false,
     "start_time": "2023-11-25T15:22:10.583159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape: (220000, 5)\n",
      "Class distribution:\n",
      "sentiment\n",
      "positive    100000\n",
      "negative     80000\n",
      "neutral      40000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample  number from each sentiment class \n",
    "pos_review = df_positive.sample(n=100000, random_state=42)\n",
    "neg_review = df_negative.sample(n=80000, random_state=42)  \n",
    "neu_review = df_neutral.sample(n=40000, random_state=42)\n",
    "\n",
    "# Concatenate them\n",
    "df_review_bal = pd.concat([pos_review, neg_review, neu_review])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_review_bal = df_review_bal.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced dataset shape:\", df_review_bal.shape)\n",
    "print(\"Class distribution:\")\n",
    "print(df_review_bal['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53f39e",
   "metadata": {
    "id": "eXErFvHFw5Ov",
    "papermill": {
     "duration": 0.005633,
     "end_time": "2023-11-25T15:22:10.617660",
     "exception": false,
     "start_time": "2023-11-25T15:22:10.612027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Splitting dataset into training and test set\n",
    "\n",
    "Before we work with our data, we need to split it into a train and test set. The train dataset will be used to fit the model, while the test dataset will be used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "We'll use ***sklearn's train_test_split*** to do the job. In this case, we set 20% to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f92e0b0",
   "metadata": {
    "id": "MsQICfa6jrng",
    "papermill": {
     "duration": 1.230412,
     "end_time": "2023-11-25T15:22:11.853992",
     "exception": false,
     "start_time": "2023-11-25T15:22:10.623580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_review_bal['Review'] = df_review_bal['Text'].astype(str)\n",
    "# Split dataset (70% train, 30% test)\n",
    "train, test = train_test_split(df_review_bal, test_size=0.2, random_state=1, stratify=df_review_bal['sentiment'])\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "train_x, train_y = train['Review'], train['sentiment']\n",
    "test_x, test_y   = test['Review'], test['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46720577",
   "metadata": {
    "id": "zDPUtgPIpHtx",
    "papermill": {
     "duration": 0.005753,
     "end_time": "2023-11-25T15:22:11.865716",
     "exception": false,
     "start_time": "2023-11-25T15:22:11.859963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Natural language processing pipeline:\n",
    "\n",
    "1.\tTokenizing sentences to break text down into sentences, words, or other units. This helps the computer process text piece by piece.\n",
    "\n",
    "3.\tRemoving stop words like \"if,\" \"but,\" \"or,\" and so on. These common words usually don't help determine sentiment.\n",
    "\n",
    "4. Normalizing words: Condensing all forms of a word into a single form. For example, \"running,\" \"runs,\" and \"ran\" all become \"run.\"\n",
    "\n",
    "5. Vectorizing text: Turning the text into a numerical representation for consumption by your classifier. This converts text data into numbers that machine learning models can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97918c20",
   "metadata": {
    "id": "ktNttUt7qfay",
    "papermill": {
     "duration": 0.005873,
     "end_time": "2023-11-25T15:22:11.877512",
     "exception": false,
     "start_time": "2023-11-25T15:22:11.871639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TF-IDF Vectorizer\n",
    "To transform review text into numerical features suitable for machine learning, we apply the Term Frequency–Inverse Document Frequency (TF-IDF) vectorizer. TF-IDF converts unstructured text into vectors that reflect the importance of words within a collection of documents.\n",
    "\n",
    "- Term Frequency (TF): Counts how often a term appears in a single document.\n",
    "\n",
    "- Document Frequency (DF): Measures in how many documents a term appears.\n",
    "\n",
    "- Inverse Document Frequency (IDF): Reduces the weight of terms that occur across many documents, since these provide little discriminatory power.\n",
    "\n",
    "By multiplying TF and IDF, the TF-IDF score highlights words that are both frequent in a given review and distinctive across the corpus. Common but uninformative words (e.g., “good,” “food,” “product”) are filtered out through stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca8fdb4",
   "metadata": {
    "id": "801Tf9pvj0s3",
    "papermill": {
     "duration": 3.792481,
     "end_time": "2023-11-25T15:22:15.675921",
     "exception": false,
     "start_time": "2023-11-25T15:22:11.883440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yikho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re #for regex text cleaning\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources (no-op if already present)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Base stopwords\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Domain stopwords (restaurant)\n",
    "domain_stopwords = {\n",
    "    'like','good','great','just','really','best','better','love',\n",
    "    'taste','flavor','food','eat','tastes',\n",
    "    'coffee','tea','chocolate','cup','sugar','water','bag','box',\n",
    "    'product','amazon','price','order','buy','bought','store','free',\n",
    "    'dog'\n",
    "}\n",
    "\n",
    "\n",
    "all_stopwords = default_stopwords.union(domain_stopwords)\n",
    "\n",
    "# Simple cleaner with basic negation handling\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'<.*?>', ' ', text)            # remove HTML tag\n",
    "    text = re.sub(r\"n['’]t\", \" not\", text)        # convert n't -> not\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)      # keep letters/space\n",
    "    text = text.lower()                           # lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()      # remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Preprocess\n",
    "train_x_cleaned = train_x.apply(clean_text)\n",
    "test_x_cleaned  = test_x.apply(clean_text)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words=list(all_stopwords),\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "train_x_vector = tfidf.fit_transform(train_x_cleaned)\n",
    "test_x_vector  = tfidf.transform(test_x_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4568e363",
   "metadata": {
    "id": "RukFHYWDxN7Q",
    "papermill": {
     "duration": 0.006033,
     "end_time": "2023-11-25T15:23:49.385257",
     "exception": false,
     "start_time": "2023-11-25T15:23:49.379224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training and Classification using Linear SVC\n",
    "## Group Member: Tang Yik Hong\n",
    "\n",
    "The Linear SVC algorithm works by:\n",
    "- Representing each document (review) as a vector of features (e.g., TF-IDF scores)\n",
    "- Finding a linear hyperplane that separates classes (positive, neutral, negative) in this high-dimensional space\n",
    "- Maximizing the margin, which is the distance between the hyperplane and the nearest data points (support vectors)\n",
    "- Using only these support vectors to define the decision boundary while ignoring less critical points\n",
    "- For multi-class problems, applying a one-vs-rest strategy, training one classifier per class and choosing the class with the highest score\n",
    "- Producing a final decision boundary that generalizes well to new, unseen reviews\n",
    "\n",
    "Each decision boundary in the Linear SVC divides the data using a linear hyperplane, separating classes while maximizing the margin between them until the optimal boundary is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba386e71",
   "metadata": {
    "id": "ykWc3ys8kUkQ",
    "outputId": "6873eb01-c1fc-4e8e-b586-47979a23ccc8",
    "papermill": {
     "duration": 12.751498,
     "end_time": "2023-11-25T15:24:02.142719",
     "exception": false,
     "start_time": "2023-11-25T15:23:49.391221",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearSVC + Calibrated probabilities model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:52<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for example reviews:\n",
      "Review: 'The pasta was incredible and the sauce tasted fresh.'\n",
      "Prediction: positive (Confidence: 0.8651)\n",
      "--------------------------------------------------\n",
      "Review: 'Shipping was slow and the snacks arrived stale.'\n",
      "Prediction: negative (Confidence: 0.6962)\n",
      "--------------------------------------------------\n",
      "Review: 'It’s okay—nothing special but not terrible either.'\n",
      "Prediction: negative (Confidence: 0.5214)\n",
      "--------------------------------------------------\n",
      "Review: 'Absolutely delicious cookies, will buy again!'\n",
      "Prediction: positive (Confidence: 0.9747)\n",
      "--------------------------------------------------\n",
      "Review: 'The seasoning was bland and the texture was mushy.'\n",
      "Prediction: negative (Confidence: 0.8757)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Base model (more iters helps convergence)\n",
    "base_svc = LinearSVC(random_state=42, max_iter=5000)\n",
    "\n",
    "# Calibrated for predict_proba\n",
    "svm_model = CalibratedClassifierCV(base_svc, cv=5, method='isotonic')  # try method='isotonic' if you have lots of data\n",
    "\n",
    "print(\"Training LinearSVC + Calibrated probabilities model...\")\n",
    "with tqdm(total=100) as pbar:\n",
    "    svm_model.fit(train_x_vector, train_y)\n",
    "    for _ in range(100):  # cosmetic progress bar\n",
    "        time.sleep(0.01)\n",
    "        pbar.update(1)\n",
    "\n",
    "def predict_with_confidence(text, model, vectorizer):\n",
    "    cleaned = clean_text(text)\n",
    "    X = vectorizer.transform([cleaned])\n",
    "    probs = model.predict_proba(X)[0]          # (n_classes,)\n",
    "    classes = model.classes_                   # e.g. ['negative','neutral','positive']\n",
    "    pred_idx = int(np.argmax(probs))\n",
    "    return classes[pred_idx], float(probs[pred_idx])\n",
    "\n",
    "# Food-domain examples\n",
    "test_reviews = [\n",
    "    \"The pasta was incredible and the sauce tasted fresh.\",\n",
    "    \"Shipping was slow and the snacks arrived stale.\",\n",
    "    \"It’s okay—nothing special but not terrible either.\",\n",
    "    \"Absolutely delicious cookies, will buy again!\",\n",
    "    \"The seasoning was bland and the texture was mushy.\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredictions for example reviews:\")\n",
    "for review in test_reviews:\n",
    "    sentiment, confidence = predict_with_confidence(review, svm_model, tfidf)\n",
    "    print(f\"Review: '{review}'\")\n",
    "    print(f\"Prediction: {sentiment} (Confidence: {confidence:.4f})\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb58a0",
   "metadata": {
    "id": "iGoxdKmdxSYH",
    "papermill": {
     "duration": 0.005967,
     "end_time": "2023-11-25T15:24:02.155204",
     "exception": false,
     "start_time": "2023-11-25T15:24:02.149237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training and Classification using Multinomial Naive Bayes Classifier\n",
    "## Group Member: Chai Ee Yuan \n",
    "The Naïve Bayes algorithm is a supervised learning method based on Bayes’ Theorem. It is widely used for text classification tasks due to its simplicity and efficiency in handling high-dimensional data like word features.\n",
    "\n",
    "The Multinomial Naive Bayes classifier works by:\n",
    "- Representing each document as a vector of word counts or TF-IDF features\n",
    "- Assuming that features (words) are conditionally independent given the class label\n",
    "- Estimating the probability of a review belonging to each class using Bayes’ Theorem\n",
    "- Selecting the class with the highest posterior probability as the predicted label\n",
    "- Performing well even with relatively small amounts of training data because of its probabilistic foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb9110b2",
   "metadata": {
    "id": "fPn6txQskYCe",
    "outputId": "11733c17-ccb9-4e57-fc06-db795a83d12f",
    "papermill": {
     "duration": 7.568245,
     "end_time": "2023-11-25T15:24:09.729692",
     "exception": false,
     "start_time": "2023-11-25T15:24:02.161447",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Multinomial Naive Bayes model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for example reviews:\n",
      "Review: 'The cookies were amazing, fresh and crispy!'\n",
      "Prediction: positive (Confidence: 0.7846)\n",
      "--------------------------------------------------\n",
      "Review: 'The chips arrived stale and the package was damaged.'\n",
      "Prediction: negative (Confidence: 0.6761)\n",
      "--------------------------------------------------\n",
      "Review: 'It was okay, nothing special but not too bad either.'\n",
      "Prediction: neutral (Confidence: 0.7201)\n",
      "--------------------------------------------------\n",
      "Review: 'Absolutely delicious chocolate. I will order again.'\n",
      "Prediction: positive (Confidence: 0.9326)\n",
      "--------------------------------------------------\n",
      "Review: 'The soup had no flavor and the noodles were soggy.'\n",
      "Prediction: negative (Confidence: 0.4319)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# ---- Train Gaussian Naive Bayes with a tqdm bar ----\n",
    "# NOTE: GaussianNB requires dense arrays; convert TF-IDF (sparse) to dense.\n",
    "X_train_dense = train_x_vector.toarray()\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "print(\"Training Multinomial Naive Bayes model...\")\n",
    "with tqdm(total=100) as pbar:\n",
    "    mnb.fit(X_train_dense, train_y)  # actual training\n",
    "    # show a 100-step bar (cosmetic; fit itself doesn't report progress)\n",
    "    for _ in range(100):\n",
    "        time.sleep(0.02)  # adjust to taste for the bar speed\n",
    "        pbar.update(1)\n",
    "\n",
    "# ---- Prediction helper (multiclass-safe) ----\n",
    "def predict_with_confidence(text, model, vectorizer):\n",
    "    cleaned = clean_text(text)\n",
    "    X = vectorizer.transform([cleaned]).toarray()\n",
    "    probs = model.predict_proba(X)[0]            # shape (n_classes,)\n",
    "    classes = model.classes_                     # e.g., ['negative','neutral','positive']\n",
    "    pred_idx = int(np.argmax(probs))\n",
    "    return classes[pred_idx], float(probs[pred_idx])\n",
    "\n",
    "# ---- Example reviews (use any list you like) ----\n",
    "test_reviews = [\n",
    "    \"The cookies were amazing, fresh and crispy!\",\n",
    "    \"The chips arrived stale and the package was damaged.\",\n",
    "    \"It was okay, nothing special but not too bad either.\",\n",
    "    \"Absolutely delicious chocolate. I will order again.\",\n",
    "    \"The soup had no flavor and the noodles were soggy.\"\n",
    "]\n",
    "\n",
    "\n",
    "# ---- Print predictions in your requested format ----\n",
    "print(\"\\nPredictions for example reviews:\")\n",
    "for review in test_reviews:\n",
    "    sentiment, confidence = predict_with_confidence(review, mnb, tfidf)\n",
    "    print(f\"Review: '{review}'\")\n",
    "    print(f\"Prediction: {sentiment} (Confidence: {confidence:.4f})\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be2284",
   "metadata": {
    "id": "egcKH2zFxXTg",
    "papermill": {
     "duration": 0.005637,
     "end_time": "2023-11-25T15:24:09.741552",
     "exception": false,
     "start_time": "2023-11-25T15:24:09.735915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training and Classification using Logistic Regression\n",
    "## Group Member: Teh Wei Zhang\n",
    "\n",
    "Logistic regression is a statistical method that predicts the probability of binary outcomes by applying the sigmoid function to a linear combination of features, converting the resulting probability (between 0 and 1) into class predictions, and optimizing model parameters using gradient descent to minimize classification errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "491233fa",
   "metadata": {
    "id": "WlrN9sZdxcfm",
    "outputId": "9ad204aa-3812-4f68-e43a-899dec717cbd",
    "papermill": {
     "duration": 1.374377,
     "end_time": "2023-11-25T15:24:11.121749",
     "exception": false,
     "start_time": "2023-11-25T15:24:09.747372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for example reviews:\n",
      "Review: 'Just saw the new Marvel movie. OMG it was amazing!! Can't wait for the sequel!'\n",
      "Prediction: positive (Confidence: 0.9945)\n",
      "--------------------------------------------------\n",
      "Review: 'What a waste of money. Boring plot, terrible acting, fell asleep halfway through.'\n",
      "Prediction: negative (Confidence: 0.9911)\n",
      "--------------------------------------------------\n",
      "Review: 'This movie literally changed my life. I've seen it 3 times already!'\n",
      "Prediction: negative (Confidence: 0.6490)\n",
      "--------------------------------------------------\n",
      "Review: 'The director should be embarrassed. Worst movie I've seen this year by far.'\n",
      "Prediction: negative (Confidence: 0.9622)\n",
      "--------------------------------------------------\n",
      "Review: 'I laughed so hard my sides hurt. This comedy is an instant classic.'\n",
      "Prediction: negative (Confidence: 0.4901)\n",
      "--------------------------------------------------\n",
      "Review: 'I wanted to walk out after 30 minutes. How did this movie even get made?'\n",
      "Prediction: negative (Confidence: 0.6043)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# (Optional) make sure the solver converges comfortably\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "\n",
    "print(\"Training Logistic Regression model...\")\n",
    "with tqdm(total=100) as pbar:\n",
    "    log_reg.fit(train_x_vector, train_y)\n",
    "    for _ in range(100):  # cosmetic bar like your example\n",
    "        time.sleep(0.01)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Multiclass-safe prediction with calibrated confidence\n",
    "def predict_with_confidence(text, model, vectorizer):\n",
    "    # clean with the SAME function you used for training\n",
    "    cleaned = clean_text(text)\n",
    "    X = vectorizer.transform([cleaned])\n",
    "    probs = model.predict_proba(X)[0]           # shape: (n_classes,)\n",
    "    classes = model.classes_                    # e.g. ['negative','neutral','positive']\n",
    "    pred_idx = int(np.argmax(probs))\n",
    "    pred_label = classes[pred_idx]\n",
    "    return pred_label, float(probs[pred_idx]), dict(zip(classes, probs))\n",
    "\n",
    "# Pretty printer to match your formatting\n",
    "def show_predictions(reviews, model, vectorizer):\n",
    "    print(\"\\nPredictions for example reviews:\")\n",
    "    for r in reviews:\n",
    "        label, conf, _ = predict_with_confidence(r, model, vectorizer)\n",
    "        print(f\"Review: '{r}'\")\n",
    "        print(f\"Prediction: {label} (Confidence: {conf:.4f})\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Examples (replace with your own)\n",
    "test_reviews = [\n",
    "    \"Just saw the new Marvel movie. OMG it was amazing!! Can't wait for the sequel!\",\n",
    "    \"What a waste of money. Boring plot, terrible acting, fell asleep halfway through.\",\n",
    "    \"This movie literally changed my life. I've seen it 3 times already!\",\n",
    "    \"The director should be embarrassed. Worst movie I've seen this year by far.\",\n",
    "    \"I laughed so hard my sides hurt. This comedy is an instant classic.\",\n",
    "    \"I wanted to walk out after 30 minutes. How did this movie even get made?\"\n",
    "]\n",
    "\n",
    "show_predictions(test_reviews, log_reg, tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0c718",
   "metadata": {
    "id": "6ODoZlyPxud0",
    "papermill": {
     "duration": 0.00746,
     "end_time": "2023-11-25T15:24:11.138003",
     "exception": false,
     "start_time": "2023-11-25T15:24:11.130543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comparing models' performance\n",
    "Using the scikit-learn library, we can evaluate the accuracy of each machine learning model on the test dataset. By printing the accuracy scores of Linear SVC, Multinomial Naive Bayes, and Logistic Regression, we can directly compare their performance on the sentiment classification task.\n",
    "\n",
    "- The model with the highest accuracy will be considered the most effective for predicting whether a food review is positive, neutral, or negative.\n",
    "- This comparison ensures we select a classifier that balances efficiency with predictive reliability for large-scale text data such as the Amazon Fine Food Reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f74d52a",
   "metadata": {
    "id": "9vJ7s5kFxt8d",
    "outputId": "0fce0860-0617-430e-f542-d6198cd58499",
    "papermill": {
     "duration": 32.308776,
     "end_time": "2023-11-25T15:24:43.468554",
     "exception": false,
     "start_time": "2023-11-25T15:24:11.159778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: 0.7827727272727273\n",
      "Multinomial Naive Bayes accuracy: 0.7357727272727272\n",
      "Logistic Regression accuracy: 0.7808636363636363\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM accuracy:\", svm_model.score(test_x_vector, test_y))\n",
    "print(\"Multinomial Naive Bayes accuracy:\", mnb.score(test_x_vector.toarray(), test_y))\n",
    "print(\"Logistic Regression accuracy:\", log_reg.score(test_x_vector, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84b6dd",
   "metadata": {
    "id": "Vw-pyJjmlUGg",
    "papermill": {
     "duration": 0.006346,
     "end_time": "2023-11-25T15:24:43.481846",
     "exception": false,
     "start_time": "2023-11-25T15:24:43.475500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# F1 scores for all models\n",
    "The F1 score is a robust evaluation metric for classification tasks, especially when dealing with imbalanced datasets. Unlike accuracy, which only measures the percentage of correct predictions, the F1 score combines both precision and recall into a single measure:\n",
    "\n",
    "- Precision: Of the reviews predicted as a given class (positive, negative, or neutral), how many were actually correct?\n",
    "\n",
    "- Recall: Of all the reviews that truly belong to a class, how many did the model correctly identify?\n",
    "\n",
    "- F1 Score: The harmonic mean of precision and recall, balancing both metrics.\n",
    "\n",
    "By generating classification reports for SVM, Multinomial Naive Bayes, and Logistic Regression, we can directly compare not just accuracy, but also how well each model balances false positives and false negatives.\n",
    "\n",
    "This evaluation provides a clearer picture of performance for food review sentiment analysis, ensuring the chosen model performs reliably across all three classes (positive, neutral, negative), rather than being biased toward the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb1cc17e-6394-4027-8240-9cc4821cc8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.82      0.89      0.85     20000\n",
      "    negative       0.78      0.84      0.81     16000\n",
      "     neutral       0.63      0.41      0.50      8000\n",
      "\n",
      "    accuracy                           0.78     44000\n",
      "   macro avg       0.74      0.71      0.72     44000\n",
      "weighted avg       0.77      0.78      0.77     44000\n",
      "\n",
      "Multinomial Naive Bayes accuracy Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.74      0.90      0.82     20000\n",
      "    negative       0.75      0.79      0.77     16000\n",
      "     neutral       0.61      0.22      0.32      8000\n",
      "\n",
      "    accuracy                           0.74     44000\n",
      "   macro avg       0.70      0.64      0.63     44000\n",
      "weighted avg       0.72      0.74      0.71     44000\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.82      0.89      0.85     20000\n",
      "    negative       0.78      0.83      0.80     16000\n",
      "     neutral       0.61      0.43      0.50      8000\n",
      "\n",
      "    accuracy                           0.78     44000\n",
      "   macro avg       0.74      0.71      0.72     44000\n",
      "weighted avg       0.77      0.78      0.77     44000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Decision Tree report\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(test_y, \n",
    "                           svm_model.predict(test_x_vector), \n",
    "                           labels=['positive', 'negative','neutral']))\n",
    "\n",
    "# Multinomial Naive Bayes accuracy\n",
    "print(\"Multinomial Naive Bayes accuracy Classification Report:\")\n",
    "print(classification_report(test_y, \n",
    "                           mnb.predict(test_x_vector.toarray()), \n",
    "                           labels=['positive', 'negative','neutral']))\n",
    "\n",
    "# For classification report\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(test_y, \n",
    "                           log_reg.predict(test_x_vector),\n",
    "                           labels=['positive', 'negative','neutral']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0db6e0",
   "metadata": {
    "id": "02pzm54a493E",
    "papermill": {
     "duration": 0.006376,
     "end_time": "2023-11-25T15:25:41.243216",
     "exception": false,
     "start_time": "2023-11-25T15:25:41.236840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Confusion Matrix for all models\n",
    "\n",
    "**A confusion matrix is a table that allows visualization of the performance of an algorithm. This table typically has two rows and two columns that report the number of false positives, false negatives, true positives, and true negatives:**\n",
    "\n",
    "- TP (True Positives): Correctly predicted positive values\n",
    "- FP (False Positives): Incorrectly predicted positive values\n",
    "- FN (False Negatives): Incorrectly predicted negative values\n",
    "- TN (True Negatives): Correctly predicted negative values\n",
    "\n",
    "Array represents: \n",
    "\n",
    "TP FN FN\n",
    "\n",
    "FP TP FN\n",
    "\n",
    "FP FN TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e7c9c31",
   "metadata": {
    "id": "FoWLOx2gls0E",
    "outputId": "0081fe69-9dd7-48cd-d2c3-5a22ef994827",
    "papermill": {
     "duration": 28.817481,
     "end_time": "2023-11-25T15:26:10.067224",
     "exception": false,
     "start_time": "2023-11-25T15:25:41.249743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Confusion Matrix:\n",
      "[[17739  1383   878]\n",
      " [ 1564 13429  1007]\n",
      " [ 2298  2428  3274]]\n",
      "Multinomial Naive Bayes Confusion Matrix:\n",
      "[[18034  1514   452]\n",
      " [ 2743 12607   650]\n",
      " [ 3472  2795  1733]]\n",
      "Logistic Regression Confusion Matrix:\n",
      "[[17736  1310   954]\n",
      " [ 1609 13207  1184]\n",
      " [ 2250  2335  3415]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# SVM confusion matrix\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(confusion_matrix(test_y, \n",
    "                      svm_model.predict(test_x_vector),\n",
    "                      labels=['positive', 'negative','neutral']))\n",
    "\n",
    "# For Multinomial Naive Bayes confusion matrix  \n",
    "print(\"Multinomial Naive Bayes Confusion Matrix:\")\n",
    "print(confusion_matrix(test_y, \n",
    "                      mnb.predict(test_x_vector.toarray()),\n",
    "                      labels=['positive', 'negative','neutral']))\n",
    "\n",
    "# For Logistic Regression confusion matrix\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(confusion_matrix(test_y, \n",
    "                       log_reg.predict(test_x_vector), \n",
    "                       labels=['positive', 'negative','neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2194a3d-c17b-41f5-9851-6de90e851e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_model, \"svm_model.pkl\")\n",
    "joblib.dump(mnb, \"mnb_model.pkl\")\n",
    "joblib.dump(log_reg, \"log_reg_model.pkl\")\n",
    "joblib.dump(tfidf, \"vectorizer.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30474,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 252.886367,
   "end_time": "2023-11-25T15:26:10.996025",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-25T15:21:58.109658",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
